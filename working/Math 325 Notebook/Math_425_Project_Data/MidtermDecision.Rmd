---
title: "Keep or Drop?"
output: 
  html_document:
    code_folding: "hide"
---

```{r setup, include = F, message = F, warning = F}
library(tidyverse)
library(ggbeeswarm)
library(car)
grades <- read_csv("data/Math425Grades.csv")
```

Our class is given the choice to keep our midterm exam score, which will be 30% of our final exam score, or have it dropped. We have to decide now; we can't wait until after. We are permitted to have the scores from the previous semester and analyze them to make our decision.

```{r, message = F, warning = F}
grades_good <- grades %>% 
  filter(FinalExam > 25)

simple_lm <- lm(FinalExam ~ Midterm, grades_good)

predictions <- data.frame(predict(simple_lm, interval = "prediction"))

ggplot(grades_good, aes(Midterm, FinalExam, shape = Gender, color = SkillsQuizzesTotal, linetype = "Lines")) +
  geom_point(size = 3, alpha = 0.75) +
  scale_colour_gradient2(low = "red", mid = "green", high = "blue", midpoint = 95) +
  geom_abline(intercept = simple_lm$coef[1], slope = simple_lm$coef[2]) +
  geom_line(aes(y = predictions$lwr), color = "black", linetype = "dashed") +
  geom_line(aes(y = predictions$upr), color = "black", linetype = "dashed") +
  #scale_linetype_manual(values = c("solid", "dashed"), labels = c("Regression Line", "Prediction Interval"))
  theme_dark() +
  geom_text(x = 54, y = 62, label = "Regression line", color = "black", size = 3.5) +
  geom_text(x = 79, y = 42, label = "95% prediction lnterval", color = "black", size = 3.5) +
  geom_vline(xintercept = 96, linetype = "dotted", color = "black") +
  geom_text(x = 95, y = 32, label = "My midterm score this semester", color = "black", size = 3, angle = 90) +
  labs(x = "Midterm Exam Score", y = "Final Exam Score", color = "Total Skills-\nQuizzes Score", title = "Scores from Last Semester's Math 425 Class") +
  scale_y_continuous(limits = c(22,100))

my_predicted_final <- predict(simple_lm, data.frame(Midterm = 96), interval = "prediction")
```
<br/>Acording to this regression model, the average final exam score is about 14.98 plus 0.765 times the midterm score.

$$
\underbrace{\hat{Y}}_\text{Final} = 14.981 + 0.765\underbrace{X_i}_\text{Midterm}
$$
The standard deviation from the fitted line is estimated at 12.43. It is important to note that this model does not include the lowest outlier scores from the data and is intended to describe the group I assumed myself to be in.

The grade I got on my midterm exam was 96%. Using the linear model, my final exam score is predicted at around `r my_predicted_final[1]`% and somewhere between `r my_predicted_final[2]`% and 100%.

It is best for me to keep this score. First, the final exam scores are less than the midterm scores on average (possibly due to students running out of time to study). Second, my midterm score is higher than all of the other ones in this dataset. Third and most importantly, a score of 96 is above 93, which separates an A from an A-. Such a score (and really any score) would only be worth dropping if my final exam score is likely to be higher. With a predicted final exam score of 88.4, I choose to keep my midterm score.

## Exploration and Diagnosis

I first plotted this:
```{r, message = F, warning = F}
ggplot(grades, aes(Midterm, FinalExam, shape = Gender, color = SkillsQuizzesTotal)) +
  geom_point()
```
<br/>And I didn't want to include the outliers (All the ones with a final exam score below 25%). My grade was 96%. It is a good grade and although I am skeptical, I will assume for this analysis that Brother Saunders standardized our midterm grades in a way that makes it appropriate to put our scores on the same scale.

I'd like to see the skills quizzes totals too:
```{r, message = F, warning = F}
ggplot(grades_good, aes(Gender, SkillsQuizzesTotal, color = FinalExam)) +
  geom_beeswarm(size = 2) +
  coord_flip()
```
<br/>Although I plan to do well on completing skills quizes, I don't feel like it's necessary to remove the outlier here.

At this point I made my linear model.
```{r, message = F, warning = F}
boxCox(simple_lm)
```
<br/>The Box-Cox function suggests that my straight line regression fits well, but I may still want to change my model because out of fear that it's invalid for scores close to 100%. Unfortunately, I spent a lot of my time on the simple linear model with ggplot and need to move on to other activities.

```{r, message = F, warning = F}
plot(simple_lm, which = 1)
qqPlot(simple_lm)
```
<br/>The assumed normality of the residuals is a little questionable. I'm not sure why the residuals are bimodal. It could just be noise, but bimodal distributions in class grades are very common. Again, I suspect that the deadlines and piled work near the end of the semester in this and other classes to be the culprit.

This is the bimodal distribution I'm talking about:
```{r, message = F, warning = F}
tibble(simple_lm$residuals, simple_lm$fitted.values) %>% 
  ggplot(aes(x = simple_lm$residuals)) +
    geom_histogram(aes(y = ..density..), alpha = 0.5, binwidth = 2, color = "gray80") +
    geom_density(position = "identity", geom = "line", fill = "blue", alpha = 0.5)
```

I could look at a new line drawn after removing the 6 data points that had the lowest residual value. This is mostly just out of curiosity, but shows me what can happen if I manage my time well, and conquer the final projects.
```{r, message = F, warning = F}
grade_no_procrastination <- grades_good %>% 
  filter(simple_lm$residuals > -10)

new_lm <- lm(FinalExam ~ Midterm, data = grade_no_procrastination)
boxCox(new_lm, xlab = "Î»\nThis looks about the same.")

new_predictions <- data.frame(predict(new_lm, interval = "prediction"))

ggplot(grade_no_procrastination, aes(Midterm, FinalExam, shape = Gender, color = SkillsQuizzesTotal, linetype = "Lines")) +
  geom_point(size = 3, alpha = 0.75) +
  scale_colour_gradient2(low = "red", mid = "green", high = "blue", midpoint = 95) +
  geom_abline(intercept = new_lm$coef[1], slope = new_lm$coef[2]) +
  geom_line(aes(y = new_predictions$lwr), color = "black", linetype = "dashed") +
  geom_line(aes(y = new_predictions$upr), color = "black", linetype = "dashed") +
  theme_dark() +
  geom_text(x = 52, y = 62, label = "Regression line", color = "black", size = 3.5) +
  geom_text(x = 78.5, y = 62.6, label = "95% prediction lnterval", color = "black", size = 3.5) +
  geom_vline(xintercept = 96, linetype = "dotted", color = "black") +
  geom_text(x = 95, y = 54, label = "My midterm score this semester", color = "black", size = 3, angle = 90) +
  labs(x = "Midterm Exam Score", y = "Final Exam Score", color = "Total Skills-\nQuizzes Score", title = "A Manual Subset of Scores from Last Semester's Math 425 Class")

my_optimistic_final <- predict(simple_lm, data.frame(Midterm = 96), interval = "prediction")
```

In this scenario (one that makes some a lot of assumptions), my final exam score is predicted at 100%, but could still be as low as `r my_optimistic_final[1]`. This is even more so where I think that the model doesn't properly reflect scores near 100%. It is interesting to note however, that after removing the supposed procrastinators, the estimated slope of the line is very close to 1.