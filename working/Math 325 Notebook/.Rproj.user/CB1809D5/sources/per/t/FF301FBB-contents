---
title: "Why No One Found Our Model"
author: "by Jim Greene"
output: 
  html_document:
    code_folding: "hide"
---

```{r setup, include=FALSE}
library(pander)
library(DT)

set.seed(122) #This ensures the randomness is the "same" everytime if you play the entire R-chunk as one entire piece of code. If you run lines separately, your data might not come out the same every time.

## To begin, decide on your sample size. (You may have to revise it later to ensure all values in your lm(...) are significant.)
n <- 300
  
## Then, create 10 X-variables using functions like rnorm(n, mean, sd), rchisq(n, df), rf(n, df1, df2), rt(n, df), rbeta(n, a, b), runif(n, a, b) or sample(c(1,0), n, replace=TRUE)...

X6 <- runif(n,0,4) 
X10 <- runif(n,0,4) 
X1 <- sample(0:1,n,replace = TRUE) 

X7 <- sample(c('John', 'Henry', 'Jessie', 'Sarah', 'Cory', 'Thor', 'Amelia', 'S C Kennedy', 'Big Max', 'Orion', 'Ferris', 'Kystystal', 'Zach', 'Jacqueline', 'Emily', 'Curious George', 'Alice', 'Francine', 'Johnny James Yogurt Jr', 'Bobo', 'Olivia', 'Marcel the Shell', '(─‿‿─)', 'Moose'),  size = n, replace = T)

X9 <- sample(rnorm(n, 9, 1.3) - seq(1, 3, length.out = n), n)/6
X2 <- 1/sample(rnorm(n, 0, 7) - seq(1, 3, length.out = n), n)/X9 + rnorm(n, -3, 1)
X9 <- -(X9 + X2 + rnorm(n, -3, 1)^2)
 
X8 <- rep(9, n)
X8[29] <- 1
 
X5 <- sample(seq(0.01, 10, length.out = n), n) + rnorm(n, 0, 0.2) + X2/3
X8 <- ifelse(X5 + rnorm(n, 1, 1) > 3.9, 1, 9)
 
X9 <- X9 + X8/9 + X2/10 
 
X7 <- paste0(X7, " - ", round(X9^2))
 
X3 <- runif(n, -1000, 1000)^X5

X4 <- 8.2 + ifelse(X9 > 2, 0   + rnorm(n, 0, .33), ifelse(X9 < -1, 1 + rnorm(n, 0, .15), ifelse(runif(n, 0, 1) > 0.5, 2  + rnorm(n, 0, .33), 3 + rnorm(n, 0, .33))))
X4 <- X4 - 2*log(X2 + 1000) + X6

a <- X2[199]
X2[199] <- X8[199]
X8[199] <- a

## Then, create betas, errors (by choosing sigma), and Y

b0 <- 1.1
b1 <- -1
b2 <- 1
b3 <- 1/2
b4 <- -1/6
b5 <- -1/24
b6 <- 1/120

sigma <- 2.3

 ################################
 # You CANNOT change this part:
 errors <- rnorm(n, 0, sigma)
 ################################ 

Y <- exp(b0 + b1*X1 + b2*X6 + b3*X1*X6^2 + b4*X6^3 + b5*X1*X6^4 + b6*X6^5 + errors)

# Load Your data into a data set:
RBdata <- data.frame(Y, X1, X2, X3, X4, X5, X6, X7, X8, X9, X10)

mylm <- lm(log(Y) ~ X6 + X1 + I(X6^2):X1 + I(X6^3) + I(X6^4):X1 + I(X6^5))
summary(mylm) 

#all p-values must be significant, except "(Intercept)"
```

**Our friends were greeted by a beautiful set of fake data. It was easy to see at a glance what was going on with the Y variable:**
```{r, warning = F, fig.width = 8.5, fig.height = 7}
palette(gray(seq(0,.85,len = nrow(RBdata))))
pairs(RBdata, pch = 16, cex = 0.75, panel = panel.smooth, col.smooth = "firebrick", col = as.factor(RBdata$Y))
```
</br>
**A raw viewing revealed tidy columns that have nothing sinister about them.**
```{r}
datatable(RBdata,  extensions = 'FixedColumns',
  options = list(
    dom = 't',
    scrollX = TRUE,
    fixedColumns = TRUE
  ))
```


**After a few basic tests, one student calmly replied with his guess at our model:**</br>
**"I spent two hours trying to figure out your data, and I couldn't find a single significant X. Great job stumping me!"**

**Our proffessor, Brother Saunders, made his final guess:**</br>
**"You’ve got me stumped. Here is my final model, $Y_i^\prime=β_0+ϵ_i\  \text{where} \  Y^\prime=log(Y)$."**

**He was close:**

### True Model
$$
  log(Y_i) = 1.1 - X_{1i} + X_{6i} + \frac{1}{2} X_{1i} {X_{6i}}^2 - \frac{1}{6} {X_{6i}}^3 - \frac{1}{24} X_{1i}{X_{6i}}^4 + \frac{1}{120} {X_{6i}}^5 + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, 2.3^2)
$$
**This is how the data was created.**

    ## Same seed to get the same data everytime
    set.seed(122)

    ## Sample size
    n <- 300
  
    ## Create 10 X-variables

    X6 <- runif(n,0,4) 
    X10 <- runif(n,0,4) 
    X1 <- sample(0:1,n,replace = TRUE) 

    X7 <- sample(c('John', 'Henry', 'Jessie', 'Sarah', 'Cory', 'Thor', 'Amelia', 'S C Kennedy', 'Big Max', 'Orion',   'Ferris', 'Kystystal', 'Zach', 'Jacqueline', 'Emily', 'Curious George', 'Alice', 'Francine', 'Johnny James Yogurt Jr', 'Bobo', 'Olivia', 'Marcel the Shell', '(─‿‿─)', 'Moose'),  size = n, replace = T)

    X9 <- sample(rnorm(n, 9, 1.3) - seq(1, 3, length.out = n), n)/6
    X2 <- 1/sample(rnorm(n, 0, 7) - seq(1, 3, length.out = n), n)/X9 + rnorm(n, -3, 1)
    X9 <- -(X9 + X2 + rnorm(n, -3, 1)^2)
 
    X8 <- rep(9, n)
    X8[29] <- 1
 
    X5 <- sample(seq(0.01, 10, length.out = n), n) + rnorm(n, 0, 0.2) + X2/3
    X8 <- ifelse(X5 + rnorm(n, 1, 1) > 3.9, 1, 9)
 
    X9 <- X9 + X8/9 + X2/10 
 
    X7 <- paste0(X7, " - ", round(X9^2))
 
    X3 <- runif(n, -1000, 1000)^X5

    X4 <- 8.2 + ifelse(X9 > 2, 0   + rnorm(n, 0, .33), ifelse(X9 < -1, 1 + rnorm(n, 0, .15), ifelse(runif(n, 0, 1) > 0.5, 2  + rnorm(n, 0, .33), 3 + rnorm(n, 0, .33))))
    X4 <- X4 - 2*log(X2 + 1000) + X6

    a <- X2[199]
    X2[199] <- X8[199]
    X8[199] <- a

    ## Then, create betas, errors, and Y

    b0 <- 1.1
    b1 <- -1
    b2 <- 1
    b3 <- 1/2
    b4 <- -1/6
    b5 <- -1/24
    b6 <- 1/120

    sigma <- 2.3

    errors <- rnorm(n, 0, sigma)

    Y <- exp(b0 + b1*X1 + b2*X6 + b3*X1*X6^2 + b4*X6^3 + b5*X1*X6^4 + b6*X6^5 + errors)

   ## Build data set

    RBdata <- data.frame(Y, X1, X2, X3, X4, X5, X6, X7, X8, X9, X10)
 
    mylm <- lm(log(Y) ~ X6 + X1 + I(X6^2):X1 + I(X6^3) + I(X6^4):X1 + I(X6^5))
    summary(mylm)

```{r}
pander(summary(mylm))
```

**The model Brother Saunders suggested looks pretty good.**



```{r, fig.width = 8.5, fig.height = 7}
lmBS_beta0 <- mean(log(RBdata$Y))

palette(c("deepskyblue2", "darkorange1"))
plot(log(Y) ~ X6, col = factor(X1), pch = 16)
curve(b0 + b2*x + b4*x^3 + b6*x^5, add = T, lwd = 2, lty = 2, col = "royalblue2")
curve(b0 + b1 + b2*x + b3*x^2 + b4*x^3 + b5*x^4 + b6*x^5, add = T, lwd = 2, lty = 2, col = "orange1")
legend(3.25, -5, legend = c("X1 = 0", "X1 = 1"),
       col = c("royalblue2", "orange1"), lty = 2, lwd = 1.8, cex = 0.8, box.lty = 0)
text(2.95, -5.45, "True lines", cex = 0.8)
abline(h = lmBS_beta0, lwd = 2)
legend(0, -5, legend = c("Saunders' guess", "Student's guess"), col = "gray50", lty = c(1,0), lwd = 1.8, cex = 0.8 ,box.lty = 0)
```

**And it generalizes well to new data:**
```{r}
## NEW SEED
set.seed(123)

## Sample size
n <- 300
  
## Create 10 X-variables

X6 <- runif(n,0,4) 
X10 <- runif(n,0,4) 
X1 <- sample(0:1,n,replace = TRUE) 

X7 <- sample(c('John', 'Henry', 'Jessie', 'Sarah', 'Cory', 'Thor', 'Amelia', 'S C Kennedy', 'Big Max', 'Orion', 'Ferris', 'Kystystal', 'Zach', 'Jacqueline', 'Emily', 'Curious George', 'Alice', 'Francine', 'Johnny James Yogurt Jr', 'Bobo', 'Olivia', 'Marcel the Shell', '(─‿‿─)', 'Moose'),  size = n, replace = T)

X9 <- sample(rnorm(n, 9, 1.3) - seq(1, 3, length.out = n), n)/6
X2 <- 1/sample(rnorm(n, 0, 7) - seq(1, 3, length.out = n), n)/X9 + rnorm(n, -3, 1)
X9 <- -(X9 + X2 + rnorm(n, -3, 1)^2)
 
X8 <- rep(9, n)
X8[29] <- 1
 
X5 <- sample(seq(0.01, 10, length.out = n), n) + rnorm(n, 0, 0.2) + X2/3
X8 <- ifelse(X5 + rnorm(n, 1, 1) > 3.9, 1, 9)
 
X9 <- X9 + X8/9 + X2/10 
 
X7 <- paste0(X7, " - ", round(X9^2))
 
X3 <- runif(n, -1000, 1000)^X5

X4 <- 8.2 + ifelse(X9 > 2, 0   + rnorm(n, 0, .33), ifelse(X9 < -1, 1 + rnorm(n, 0, .15), ifelse(runif(n, 0, 1) > 0.5, 2  + rnorm(n, 0, .33), 3 + rnorm(n, 0, .33))))
X4 <- X4 - 2*log(X2 + 1000) + X6

a <- X2[199]
X2[199] <- X8[199]
X8[199] <- a

## Then, create betas, errors, and Y

b0 <- 1.1
b1 <- -1
b2 <- 1
b3 <- 1/2
b4 <- -1/6
b5 <- -1/24
b6 <- 1/120

sigma <- 2.3

errors <- rnorm(n, 0, sigma)

Y <- exp(b0 + b1*X1 + b2*X6 + b3*X1*X6^2 + b4*X6^3 + b5*X1*X6^4 + b6*X6^5 + errors)

## Build data set

new_RBdata <- data.frame(Y, X1, X2, X3, X4, X5, X6, X7, X8, X9, X10)
```
```{r, fig.width = 8.5, fig.height = 7}
plot(log(Y) ~ X6, col = factor(X1), pch = 16, data = new_RBdata)
curve(b0 + b2*x + b4*x^3 + b6*x^5, add = T, lwd = 2, lty = 2, col = "royalblue2")
curve(b0 + b1 + b2*x + b3*x^2 + b4*x^3 + b5*x^4 + b6*x^5, add = T, lwd = 2, lty = 2, col = "orange1")
legend(3.25, -4, legend = c("X1 = 0", "X1 = 1"),
       col = c("royalblue2", "orange1"), lty = 2, lwd = 1.8, cex = 0.8, box.lty = 0)
text(2.95, -4.5, "True lines", cex = 0.8)
abline(h = lmBS_beta0, lwd = 2)
legend(0.6, -4, legend = c("Saunders' guess", "Student's guess"), col = "gray50", lty = c(1,0), lwd = 1.8, cex = 0.8, box.lty = 0)
```

**Unfortunately the plot reveals that the student did not do as well as Brother Saunders. Their line does not appear anywhere near the true line.**

```{r}
trueLM_2 <- lm(log(Y) ~ X6 + X1 + I(X6^2):X1 + I(X6^3) + I(X6^4):X1 + I(X6^5))

lmBS <- lm(log(Y) ~ rep(lmBS_beta0, length(Y)), data = RBdata)

yhat_i_pred <- predict(lmBS, newdata = new_RBdata)
ybar_pred <- mean(log(new_RBdata$Y))
y_i_pred <- new_RBdata$Y

SSE_pred <- sum((y_i_pred - yhat_i_pred)^2)
SSTO_pred <- sum((y_i_pred - ybar_pred)^2)

R2_pred <- 1 - (SSE_pred/SSTO_pred)
```

**R-Squared values of the original data:**

| Model | R-squared |
|-------|--------------------|
| True | `r summary(mylm)$r.squared` |
| Brother Saunders | `r summary(lmBS)$r.squared` |
| Student | 0 |

**R-Squared values of the new data:**

| Model | R-squared |
|-------|--------------------|
| True | `r summary(trueLM_2)$r.squared` |
| Brother Saunders | -3.169481e-05 |
| Student | 0 |